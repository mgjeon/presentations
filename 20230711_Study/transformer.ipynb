{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Transformer\"\n",
    "subtitle: \"Attention Is All You Need\"\n",
    "author: \"전민규\"\n",
    "jupyter: python3\n",
    "fontsize: 1.5em\n",
    "format: \n",
    "    revealjs:\n",
    "        controls: true\n",
    "        slide-number: true\n",
    "        show-slide-number: all\n",
    "        embed-resources: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Simplified\n",
    "\n",
    "::: {style=\"text-align: center\"}\n",
    "![](Figures/the_transformer_3.png){width=\"60%\"}\n",
    ":::\n",
    "\n",
    "::: {style=\"text-align: center\" layout-ncol=2}\n",
    "![](Figures/The_transformer_encoders_decoders.png){.fragment}\n",
    "\n",
    "![](Figures/The_transformer_encoder_decoder_stack.png){.fragment}\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer in Detail\n",
    "\n",
    ":::{layout=\"[12, 20]\" layout-valign=\"center\"}\n",
    "![](Figures/model.png){fig-align=\"center\"}\n",
    "\n",
    "![](Figures/transformer-encoder-decoder.png)\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ## Encoder & Decoder\n",
    "\n",
    "![](Figures/Transformer_decoder.png){.absolute top=25% width=\"100%\"} -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ## GPT\n",
    "\n",
    "::: {style=\"text-align: center\"}\n",
    "![](Figures/gpt2-sizes-hyperparameters-3.png){width=\"1000\"}\n",
    "::: -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ## Word2Vec $\\subset$ Word Embedding\n",
    "\n",
    "::: {style=\"text-align: center\"}\n",
    "![](Figures/word2vec.png){width=\"40%\"}\n",
    "\n",
    "![](Figures/encoder_with_tensors_2.png){width=\"50%\" .fragment}\n",
    "::: -->\n",
    "\n",
    "## Word Embedding\n",
    "![](Figures/https___www.datocms-assets.com_96965_1684227697-4-transformers-explained.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "![](Figures/word-embeddings-tokenization-example.webp){fig-align=\"center\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoding\n",
    "![](Figures/https___www.datocms-assets.com_96965_1684228093-7-transformers-explained.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoding (Intuition)\n",
    "![](Figures/positional_encoding.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention\n",
    "\n",
    "![](Figures/attention_2.png){fig-align=\"center\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ## Attention / Self-Attention\n",
    "\n",
    "![](Figures/attention_2.png){.absolute top=10% right=50% width=\"50%\"}\n",
    "\n",
    "![](Figures/https___www.datocms-assets.com_96965_1684228393-8-transformers-explained.png){.absolute top=26% right=0 width=\"50%\"} -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Attention\n",
    "\n",
    "::: {style=\"text-align: center\"}\n",
    "The animal didn't cross the street because **it** was too tired. \n",
    ":::\n",
    "\n",
    "\n",
    "![](Figures/t2tattention.png){width=\"50%\" fig-align=\"center\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Attention (1) - Query, Key, Value\n",
    "\n",
    "![](Figures/self-attention-1.png){.absolute top=12% right=0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Attention (2) - Attention Score \n",
    "\n",
    "![](Figures/self-attention-2.png){.absolute top=12% right=0}\n",
    "\n",
    "::: {.absolute top=-5% right=8% width=30%}\n",
    "$$\n",
    "\\text{softmax}(\\alpha_i) = \\frac{e^{\\alpha_i}}{\\displaystyle\\sum_{k}e^{\\alpha_k}}\n",
    "$$ \n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Attention (2) - Attention Score (Intuition) {.scrollable}\n",
    "\n",
    "![](Figures/self-attention-example-folders-3.png)\n",
    "\n",
    "![](Figures/self-attention-example-folders-scores-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Attention (3) - Weighted Sum\n",
    "\n",
    "![](Figures/self-attention-3-2.png){.absolute top=12% right=0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Attention - Matrix Multiflication\n",
    "\n",
    "![](Figures/self-attention-matrix-calculation.png){.absolute top=20% left=3% width=40%}\n",
    "\n",
    "![](Figures/self-attention-matrix-calculation-2.png){.absolute top=10% right=-5% width=65%}\n",
    "\n",
    "![](Figures/matrowcomb.png){.absolute top=50% right=10% width=30%}\n",
    "\n",
    "::: {.absolute top=-5% right=8% width=30%}\n",
    "$$\n",
    "\\text{softmax}(\\alpha_i) = \\frac{e^{\\alpha_i}}{\\displaystyle\\sum_{k}e^{\\alpha_k}}\n",
    "$$ \n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Head Attention - Q/K/V\n",
    "\n",
    "![](Figures/transformer_attention_heads_qkv.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Head Attention - Z {.scrollable}\n",
    "\n",
    "![](Figures/transformer_attention_heads_z.png)\n",
    "\n",
    "![](Figures/t2tattention_2.png){fig-align=\"center\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Head Attention - Concat\n",
    "\n",
    "![](Figures/transformer_attention_heads_weight_matrix_o.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Head Attention - Summary\n",
    "\n",
    "![](Figures/transformer_multi-headed_self-attention-recap.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "\n",
    "![](Figures/encoder_with_tensors_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ## Positional Encoding\n",
    "\n",
    "![](Figures/transformer_positional_encoding_vectors.png) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder in Detail - Residual connection & Layer normalization\n",
    "\n",
    "![](Figures/transformer_resideual_layer_norm_2.png){fig-align=\"center\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer normalization\n",
    "\n",
    "![](Figures/Picture1 (2).png){fig-align=\"center\" width=\"45%\"}\n",
    "\n",
    "![](Figures/Picture1.png){fig-align=\"center\" width=\"45%\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Position-wise Feed Forward Neural Network\n",
    "\n",
    "$$\\begin{align}\n",
    "\\text{FFN}(x) & = \\text{ReLU}(xW_1 + b_1)W_2 + b_2 \\\\\n",
    "              & = \\text{max}(0, xW_1 + b_1)W_2 + b_2\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "![](Figures/pffn.png){fig-align=\"center\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder & Decoder\n",
    "\n",
    "![](Figures/transformer_resideual_layer_norm_3.png){fig-align=\"center\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder - Parallelized Predictions During Training\n",
    "\n",
    "- Feed entire target sequence into decoder\n",
    "    - SOS (**S**tart **O**f a **S**equnce)\n",
    "    - EOS (**E**nd **O**f a **S**equence)\n",
    "\n",
    "![](Figures/decoder_training.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder - Masked Multi-Head Self-Attention\n",
    "\n",
    "![](Figures/decoder_model.png){fig-align=\"center\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masked Self-Attention (1) - Query, Key, Value\n",
    "![](Figures/https___www.datocms-assets.com_96965_1684229494-11-transformers-explained.png){fig-align=\"center\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masked Self-Attention (2) - Attention Score\n",
    "![](Figures/https___www.datocms-assets.com_96965_1684229734-12-transformers-explained.png){width=\"80%\" .absolute top=10% right=8%}\n",
    "\n",
    "<!-- ![](Figures/transformer-attention-mask.png){width=\"80%\" .absolute top=60% right=8%} -->\n",
    "\n",
    "![](Figures/multi_head.png){width=\"30%\" .absolute top=60% right=35%}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masked Self-Attention (3) - Masked Attention Score\n",
    "\n",
    "![](Figures/transformer-attention-mask.png){width=\"80%\" .absolute top=10% right=8%}\n",
    "<!-- ![](Figures/transformer-attention-masked-scores-softmax.png){width=\"80%\" .absolute top=20% right=8%} -->\n",
    "\n",
    "<!-- ![](Figures/self-attention-matrix-calculation-2.png){.absolute top=60% right=12% width=70%} -->\n",
    "\n",
    "![](Figures/multi_head_2.png){.absolute top=50% right=12% width=70%}\n",
    "\n",
    "<!-- ::: {.absolute top=10% right=35%}\n",
    "$$\n",
    "\\text{softmax}(\\alpha_i) = \\frac{e^{\\alpha_i}}{\\displaystyle\\sum_{k}e^{\\alpha_k}}\n",
    "$$ \n",
    "::: -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masked Self-Attention (4) - Softmax\n",
    "\n",
    "<!-- ![](Figures/transformer-attention-mask.png){width=\"80%\" .absolute top=20% right=8%} -->\n",
    "![](Figures/transformer-attention-masked-scores-softmax.png){width=\"80%\" .absolute top=20% right=8%}\n",
    "\n",
    "<!-- ![](Figures/self-attention-matrix-calculation-2.png){.absolute top=60% right=12% width=70%} -->\n",
    "\n",
    "![](Figures/multi_head_3.png){.absolute top=60% right=22% width=50%}\n",
    "\n",
    "::: {.absolute top=10% right=35%}\n",
    "$$\n",
    "\\text{softmax}(\\alpha_i) = \\frac{e^{\\alpha_i}}{\\displaystyle\\sum_{k}e^{\\alpha_k}}\n",
    "$$ \n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masked Self-Attention (5)\n",
    "\n",
    "![](Figures/masked-self-attention-2.png){.absolute top=15% right=-5% width=100%}\n",
    "\n",
    "<!-- ![](Figures/transformer-decoder-block-self-attention-2.png){.absolute top=35% right=10% width=80% .fragment} -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masked Self-Attention (6)\n",
    "\n",
    "![](Figures/transformer-decoder-block-self-attention-2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masked Self-Attention (7)\n",
    "\n",
    "![](Figures/self-attention-and-masked-self-attention.png){.absolute top=25%}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masked Self-Attention (8)\n",
    "\n",
    "![](Figures/maksed_mha.png){fig-align=\"center\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder-Decoder Attention (1)\n",
    "\n",
    "![](Figures/decoder_model.png){.absolute top=15% right=60% width=40%}\n",
    "\n",
    "::: {.absolute top=10% right=0%}\n",
    "- Queries are computed using $X_D$ (Decoder input)\n",
    "    - $Q = W^Q X_D$\n",
    "- Keys and Values are computed using $X_E$ (Encoder output)\n",
    "    - $K = W^K X_E$\n",
    "    - $V = W^V X_E$\n",
    ":::\n",
    "\n",
    "![](Figures/self-attention-example-folders-3.png){.absolute top=50% right=0% width=60%}\n",
    "\n",
    "\n",
    "\n",
    "<!-- ![](Figures/transformer-decoder-block-self-attention-2.png){.absolute top=35% right=10% width=80% .fragment} -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder-Decoder Attention (2)\n",
    "\n",
    "![](Figures/https___www.datocms-assets.com_96965_1684240607-22-transformers-explained.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder-Decoder Attention (3)\n",
    "\n",
    "![](Figures/encoder_decoder_a.png){fig-align=\"center\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear and Softmax Layer\n",
    "\n",
    "![](Figures/decoder_model.png){.absolute top=20% right=72% width=30%}\n",
    "\n",
    "![](Figures/transformer_decoder_output_softmax.png){.absolute top=20% right=-2% width=70%}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Vocabulary (1)\n",
    "\n",
    "![](Figures/one-hot-vocabulary-example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Vocabulary (2)\n",
    "\n",
    ":::{layout=\"[1, 1]\"}\n",
    "![](Figures/output_target_probability_distributions.png)\n",
    "\n",
    "![](Figures/output_trained_model_probability_distributions.png)\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "![](Figures/model.png){.absolute top=5% right=55% width=45%}\n",
    "\n",
    "![](Figures/attentions.png){.absolute top=30% right=0% width=50%}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "{{< video https://www.youtube.com/watch?v=4Bdc55j80l8 width=\"100%\" height=\"100%\" >}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n",
    "\n",
    "- [Tensor2Tensor Notebook](https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb)\n",
    "\n",
    "- [Jay Alammar's Blog](https://jalammar.github.io)\n",
    "\n",
    "- [고려대학교 산업경영공학부 DSBA 연구실](https://www.youtube.com/watch?v=Yk1tV_cXMMU)\n",
    "\n",
    "- [Visualizing and Explaining Transformer Models From the Ground Up](https://deepgram.com/learn/visualizing-and-explaining-transformer-models-from-the-ground-up)\n",
    "\n",
    "- [How does Layer Normalization work?](https://www.kaggle.com/code/halflingwizard/how-does-layer-normalization-work)\n",
    "\n",
    "- [A series of videos on the transformer](https://www.youtube.com/playlist?list=PLDw5cZwIToCvXLVY2bSqt7F2gu8y-Rqje)\n",
    "\n",
    "- [Hugging Face Transformers](https://huggingface.co/docs/transformers)\n",
    "\n",
    "- [Hugging Face NLP Course](https://huggingface.co/learn/nlp-course)\n",
    "\n",
    "\n",
    "\n",
    "<!-- \n",
    "- [The Annotated Transformer](https://nlp.seas.harvard.edu/annotated-transformer/)\n",
    "\n",
    "- Youtube (Korean)\n",
    "    - <https://www.youtube.com/watch?v=Yk1tV_cXMMU>\n",
    "    - <https://www.youtube.com/watch?v=AA621UofTUA>\n",
    "    - <https://www.youtube.com/watch?v=mxGCEWOxfe8>\n",
    "\n",
    "- Youtube (English)\n",
    "    - <https://www.youtube.com/watch?v=4Bdc55j80l8> -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch implementation of Transformer\n",
    "\n",
    "- [The Annotated Transformer](https://nlp.seas.harvard.edu/annotated-transformer/)\n",
    "\n",
    "- [Attention is all you need: A Pytorch Implementation](https://github.com/jadore801120/attention-is-all-you-need-pytorch)\n",
    "\n",
    "- [pytorch-original-transformer](https://github.com/gordicaleksa/pytorch-original-transformer)\n",
    "\n",
    "- [Pytorch Transformers from Scratch (Attention is all you need)](https://www.youtube.com/watch?v=U0s0f995w14)\n",
    "\n",
    "- [[딥러닝 기계 번역] Transformer: Attention Is All You Need (꼼꼼한 딥러닝 논문 리뷰와 코드 실습)](https://www.youtube.com/watch?v=AA621UofTUA&t=3166s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quarto",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
